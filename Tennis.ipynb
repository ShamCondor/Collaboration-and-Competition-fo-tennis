{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the third project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program.\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing the necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Tennis.app\"`\n",
    "- **Windows** (x86): `\"path/to/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Tennis_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Tennis.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Tennis.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"/home/sfy/deep-reinforcement-learning/p3_collab-compet/Tennis_Linux/Tennis.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1.  If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01.  Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "[[ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.65278625 -1.5\n",
      "  -0.          0.          6.83172083  6.         -0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.         -6.4669857  -1.5\n",
      "   0.          0.         -6.83172083  6.          0.          0.        ]]\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "print(states)\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agents and receive feedback from the environment.\n",
    "\n",
    "Once this cell is executed, you will watch the agents' performance, if they select actions at random with each time step.  A window should pop up that allows you to observe the agents.\n",
    "\n",
    "Of course, as part of the project, you'll have to change the code so that the agents are able to use their experiences to gradually choose better actions when interacting with the environment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1, 6):                                      # play game for 5 episodes\n",
    "#    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "#    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "#    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "#    while True:\n",
    "#        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "#        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "##        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "#        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "#        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "#        dones = env_info.local_done                        # see if episode finished\n",
    "#        scores += env_info.rewards                         # update the score (for each agent)\n",
    "#        states = next_states                               # roll over states to next time step\n",
    "#        if np.any(dones):                                  # exit loop if episode finished\n",
    "#            break\n",
    " #   print('Score (max over agents) from episode {}: {}'.format(i, np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. It's Your Turn!\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tTotal Average Score: 0.00\tDuration: 0.58\n",
      "Episode 200\tTotal Average Score: 0.03\tDuration: 1.61\n",
      "Episode 300\tTotal Average Score: 0.05\tDuration: 0.46\n",
      "Episode 400\tTotal Average Score: 0.07\tDuration: 1.10\n",
      "Episode 500\tTotal Average Score: 0.07\tDuration: 0.44\n",
      "Episode 600\tTotal Average Score: 0.10\tDuration: 1.05\n",
      "Episode 700\tTotal Average Score: 0.13\tDuration: 1.59\n",
      "Episode 800\tTotal Average Score: 0.20\tDuration: 2.52\n",
      "Episode 900\tTotal Average Score: 0.32\tDuration: 1.04\n",
      "Episode 982\tAverage Score: 0.81\n",
      "Environment solved in 982 episodes!\tAverage Score: 0.81\n",
      "Episode 983\tAverage Score: 0.82\n",
      "Environment solved in 983 episodes!\tAverage Score: 0.82\n",
      "Episode 984\tAverage Score: 0.83\n",
      "Environment solved in 984 episodes!\tAverage Score: 0.83\n",
      "Episode 985\tAverage Score: 0.84\n",
      "Environment solved in 985 episodes!\tAverage Score: 0.84\n",
      "Episode 986\tAverage Score: 0.84\n",
      "Environment solved in 986 episodes!\tAverage Score: 0.84\n",
      "Episode 987\tAverage Score: 0.86\n",
      "Environment solved in 987 episodes!\tAverage Score: 0.86\n",
      "Episode 988\tAverage Score: 0.87\n",
      "Environment solved in 988 episodes!\tAverage Score: 0.87\n",
      "Episode 989\tAverage Score: 0.87\n",
      "Environment solved in 989 episodes!\tAverage Score: 0.87\n",
      "Episode 990\tAverage Score: 0.88\n",
      "Environment solved in 990 episodes!\tAverage Score: 0.88\n",
      "Episode 991\tAverage Score: 0.88\n",
      "Environment solved in 991 episodes!\tAverage Score: 0.88\n",
      "Episode 992\tAverage Score: 0.88\n",
      "Environment solved in 992 episodes!\tAverage Score: 0.88\n",
      "Episode 993\tAverage Score: 0.87\n",
      "Environment solved in 993 episodes!\tAverage Score: 0.87\n",
      "Episode 994\tAverage Score: 0.86\n",
      "Environment solved in 994 episodes!\tAverage Score: 0.86\n",
      "Episode 995\tAverage Score: 0.86\n",
      "Environment solved in 995 episodes!\tAverage Score: 0.86\n",
      "Episode 996\tAverage Score: 0.87\n",
      "Environment solved in 996 episodes!\tAverage Score: 0.87\n",
      "Episode 997\tAverage Score: 0.87\n",
      "Environment solved in 997 episodes!\tAverage Score: 0.87\n",
      "Episode 998\tAverage Score: 0.87\n",
      "Environment solved in 998 episodes!\tAverage Score: 0.87\n",
      "Episode 999\tAverage Score: 0.87\n",
      "Environment solved in 999 episodes!\tAverage Score: 0.87\n",
      "Episode 1000\tTotal Average Score: 0.89\tDuration: 30.90\n",
      "\n",
      "Environment solved in 1000 episodes!\tAverage Score: 0.89\n",
      "Episode 1001\tAverage Score: 0.90\n",
      "Environment solved in 1001 episodes!\tAverage Score: 0.90\n",
      "Episode 1002\tAverage Score: 0.89\n",
      "Environment solved in 1002 episodes!\tAverage Score: 0.89\n",
      "Episode 1003\tAverage Score: 0.92\n",
      "Environment solved in 1003 episodes!\tAverage Score: 0.92\n",
      "Episode 1004\tAverage Score: 0.93\n",
      "Environment solved in 1004 episodes!\tAverage Score: 0.93\n",
      "Episode 1005\tAverage Score: 0.95\n",
      "Environment solved in 1005 episodes!\tAverage Score: 0.95\n",
      "Episode 1006\tAverage Score: 0.95\n",
      "Environment solved in 1006 episodes!\tAverage Score: 0.95\n",
      "Episode 1007\tAverage Score: 0.95\n",
      "Environment solved in 1007 episodes!\tAverage Score: 0.95\n",
      "Episode 1008\tAverage Score: 0.94\n",
      "Environment solved in 1008 episodes!\tAverage Score: 0.94\n",
      "Episode 1009\tAverage Score: 0.94\n",
      "Environment solved in 1009 episodes!\tAverage Score: 0.94\n",
      "Episode 1010\tAverage Score: 0.94\n",
      "Environment solved in 1010 episodes!\tAverage Score: 0.94\n",
      "Episode 1011\tAverage Score: 0.93\n",
      "Environment solved in 1011 episodes!\tAverage Score: 0.93\n",
      "Episode 1012\tAverage Score: 0.92\n",
      "Environment solved in 1012 episodes!\tAverage Score: 0.92\n",
      "Episode 1013\tAverage Score: 0.93\n",
      "Environment solved in 1013 episodes!\tAverage Score: 0.93\n",
      "Episode 1014\tAverage Score: 0.91\n",
      "Environment solved in 1014 episodes!\tAverage Score: 0.91\n",
      "Episode 1015\tAverage Score: 0.91\n",
      "Environment solved in 1015 episodes!\tAverage Score: 0.91\n",
      "Episode 1016\tAverage Score: 0.91\n",
      "Environment solved in 1016 episodes!\tAverage Score: 0.91\n",
      "Episode 1017\tAverage Score: 0.90\n",
      "Environment solved in 1017 episodes!\tAverage Score: 0.90\n",
      "Episode 1018\tAverage Score: 0.92\n",
      "Environment solved in 1018 episodes!\tAverage Score: 0.92\n",
      "Episode 1019\tAverage Score: 0.93\n",
      "Environment solved in 1019 episodes!\tAverage Score: 0.93\n",
      "Episode 1020\tAverage Score: 0.93\n",
      "Environment solved in 1020 episodes!\tAverage Score: 0.93\n",
      "Episode 1021\tAverage Score: 0.93\n",
      "Environment solved in 1021 episodes!\tAverage Score: 0.93\n",
      "Episode 1022\tAverage Score: 0.93\n",
      "Environment solved in 1022 episodes!\tAverage Score: 0.93\n",
      "Episode 1023\tAverage Score: 0.92\n",
      "Environment solved in 1023 episodes!\tAverage Score: 0.92\n",
      "Episode 1024\tAverage Score: 0.92\n",
      "Environment solved in 1024 episodes!\tAverage Score: 0.92\n",
      "Episode 1025\tAverage Score: 0.91\n",
      "Environment solved in 1025 episodes!\tAverage Score: 0.91\n",
      "Episode 1026\tAverage Score: 0.91\n",
      "Environment solved in 1026 episodes!\tAverage Score: 0.91\n",
      "Episode 1027\tAverage Score: 0.91\n",
      "Environment solved in 1027 episodes!\tAverage Score: 0.91\n",
      "Episode 1028\tAverage Score: 0.91\n",
      "Environment solved in 1028 episodes!\tAverage Score: 0.91\n",
      "Episode 1029\tAverage Score: 0.89\n",
      "Environment solved in 1029 episodes!\tAverage Score: 0.89\n",
      "Episode 1030\tAverage Score: 0.89\n",
      "Environment solved in 1030 episodes!\tAverage Score: 0.89\n",
      "Episode 1031\tAverage Score: 0.88\n",
      "Environment solved in 1031 episodes!\tAverage Score: 0.88\n",
      "Episode 1032\tAverage Score: 0.89\n",
      "Environment solved in 1032 episodes!\tAverage Score: 0.89\n",
      "Episode 1033\tAverage Score: 0.88\n",
      "Environment solved in 1033 episodes!\tAverage Score: 0.88\n",
      "Episode 1034\tAverage Score: 0.89\n",
      "Environment solved in 1034 episodes!\tAverage Score: 0.89\n",
      "Episode 1035\tAverage Score: 0.89\n",
      "Environment solved in 1035 episodes!\tAverage Score: 0.89\n",
      "Episode 1036\tAverage Score: 0.88\n",
      "Environment solved in 1036 episodes!\tAverage Score: 0.88\n",
      "Episode 1037\tAverage Score: 0.90\n",
      "Environment solved in 1037 episodes!\tAverage Score: 0.90\n",
      "Episode 1038\tAverage Score: 0.90\n",
      "Environment solved in 1038 episodes!\tAverage Score: 0.90\n",
      "Episode 1039\tAverage Score: 0.91\n",
      "Environment solved in 1039 episodes!\tAverage Score: 0.91\n",
      "Episode 1040\tAverage Score: 0.91\n",
      "Environment solved in 1040 episodes!\tAverage Score: 0.91\n",
      "Episode 1041\tAverage Score: 0.90\n",
      "Environment solved in 1041 episodes!\tAverage Score: 0.90\n",
      "Episode 1042\tAverage Score: 0.89\n",
      "Environment solved in 1042 episodes!\tAverage Score: 0.89\n",
      "Episode 1043\tAverage Score: 0.90\n",
      "Environment solved in 1043 episodes!\tAverage Score: 0.90\n",
      "Episode 1044\tAverage Score: 0.90\n",
      "Environment solved in 1044 episodes!\tAverage Score: 0.90\n",
      "Episode 1045\tAverage Score: 0.92\n",
      "Environment solved in 1045 episodes!\tAverage Score: 0.92\n",
      "Episode 1046\tAverage Score: 0.95\n",
      "Environment solved in 1046 episodes!\tAverage Score: 0.95\n",
      "Episode 1047\tAverage Score: 0.95\n",
      "Environment solved in 1047 episodes!\tAverage Score: 0.95\n",
      "Episode 1048\tAverage Score: 0.95\n",
      "Environment solved in 1048 episodes!\tAverage Score: 0.95\n",
      "Episode 1049\tAverage Score: 0.93\n",
      "Environment solved in 1049 episodes!\tAverage Score: 0.93\n",
      "Episode 1050\tAverage Score: 0.91\n",
      "Environment solved in 1050 episodes!\tAverage Score: 0.91\n",
      "Episode 1051\tAverage Score: 0.90\n",
      "Environment solved in 1051 episodes!\tAverage Score: 0.90\n",
      "Episode 1052\tAverage Score: 0.88\n",
      "Environment solved in 1052 episodes!\tAverage Score: 0.88\n",
      "Episode 1053\tAverage Score: 0.89\n",
      "Environment solved in 1053 episodes!\tAverage Score: 0.89\n",
      "Episode 1054\tAverage Score: 0.88\n",
      "Environment solved in 1054 episodes!\tAverage Score: 0.88\n",
      "Episode 1055\tAverage Score: 0.90\n",
      "Environment solved in 1055 episodes!\tAverage Score: 0.90\n",
      "Episode 1056\tAverage Score: 0.90\n",
      "Environment solved in 1056 episodes!\tAverage Score: 0.90\n",
      "Episode 1057\tAverage Score: 0.90\n",
      "Environment solved in 1057 episodes!\tAverage Score: 0.90\n",
      "Episode 1058\tAverage Score: 0.93\n",
      "Environment solved in 1058 episodes!\tAverage Score: 0.93\n",
      "Episode 1059\tAverage Score: 0.93\n",
      "Environment solved in 1059 episodes!\tAverage Score: 0.93\n",
      "Episode 1060\tAverage Score: 0.96\n",
      "Environment solved in 1060 episodes!\tAverage Score: 0.96\n",
      "Episode 1061\tAverage Score: 0.97\n",
      "Environment solved in 1061 episodes!\tAverage Score: 0.97\n",
      "Episode 1062\tAverage Score: 0.99\n",
      "Environment solved in 1062 episodes!\tAverage Score: 0.99\n",
      "Episode 1063\tAverage Score: 0.99\n",
      "Environment solved in 1063 episodes!\tAverage Score: 0.99\n",
      "Episode 1064\tAverage Score: 0.99\n",
      "Environment solved in 1064 episodes!\tAverage Score: 0.99\n",
      "Episode 1065\tAverage Score: 0.99\n",
      "Environment solved in 1065 episodes!\tAverage Score: 0.99\n",
      "Episode 1066\tAverage Score: 0.99\n",
      "Environment solved in 1066 episodes!\tAverage Score: 0.99\n",
      "Episode 1067\tAverage Score: 0.97\n",
      "Environment solved in 1067 episodes!\tAverage Score: 0.97\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1068\tAverage Score: 0.99\n",
      "Environment solved in 1068 episodes!\tAverage Score: 0.99\n",
      "Episode 1069\tAverage Score: 0.97\n",
      "Environment solved in 1069 episodes!\tAverage Score: 0.97\n",
      "Episode 1070\tAverage Score: 0.97\n",
      "Environment solved in 1070 episodes!\tAverage Score: 0.97\n",
      "Episode 1071\tAverage Score: 0.94\n",
      "Environment solved in 1071 episodes!\tAverage Score: 0.94\n",
      "Episode 1072\tAverage Score: 0.92\n",
      "Environment solved in 1072 episodes!\tAverage Score: 0.92\n",
      "Episode 1073\tAverage Score: 0.94\n",
      "Environment solved in 1073 episodes!\tAverage Score: 0.94\n",
      "Episode 1074\tAverage Score: 0.92\n",
      "Environment solved in 1074 episodes!\tAverage Score: 0.92\n",
      "Episode 1075\tAverage Score: 0.90\n",
      "Environment solved in 1075 episodes!\tAverage Score: 0.90\n",
      "Episode 1076\tAverage Score: 0.87\n",
      "Environment solved in 1076 episodes!\tAverage Score: 0.87\n",
      "Episode 1077\tAverage Score: 0.89\n",
      "Environment solved in 1077 episodes!\tAverage Score: 0.89\n",
      "Episode 1078\tAverage Score: 0.88\n",
      "Environment solved in 1078 episodes!\tAverage Score: 0.88\n",
      "Episode 1079\tAverage Score: 0.87\n",
      "Environment solved in 1079 episodes!\tAverage Score: 0.87\n",
      "Episode 1080\tAverage Score: 0.88\n",
      "Environment solved in 1080 episodes!\tAverage Score: 0.88\n",
      "Episode 1081\tAverage Score: 0.89\n",
      "Environment solved in 1081 episodes!\tAverage Score: 0.89\n",
      "Episode 1082\tAverage Score: 0.86\n",
      "Environment solved in 1082 episodes!\tAverage Score: 0.86\n",
      "Episode 1083\tAverage Score: 0.86\n",
      "Environment solved in 1083 episodes!\tAverage Score: 0.86\n",
      "Episode 1084\tAverage Score: 0.85\n",
      "Environment solved in 1084 episodes!\tAverage Score: 0.85\n",
      "Episode 1085\tAverage Score: 0.84\n",
      "Environment solved in 1085 episodes!\tAverage Score: 0.84\n",
      "Episode 1086\tAverage Score: 0.84\n",
      "Environment solved in 1086 episodes!\tAverage Score: 0.84\n",
      "Episode 1087\tAverage Score: 0.82\n",
      "Environment solved in 1087 episodes!\tAverage Score: 0.82\n",
      "Episode 1088\tAverage Score: 0.83\n",
      "Environment solved in 1088 episodes!\tAverage Score: 0.83\n",
      "Episode 1089\tAverage Score: 0.83\n",
      "Environment solved in 1089 episodes!\tAverage Score: 0.83\n",
      "Episode 1090\tAverage Score: 0.83\n",
      "Environment solved in 1090 episodes!\tAverage Score: 0.83\n",
      "Episode 1091\tAverage Score: 0.85\n",
      "Environment solved in 1091 episodes!\tAverage Score: 0.85\n",
      "Episode 1092\tAverage Score: 0.87\n",
      "Environment solved in 1092 episodes!\tAverage Score: 0.87\n",
      "Episode 1093\tAverage Score: 0.88\n",
      "Environment solved in 1093 episodes!\tAverage Score: 0.88\n",
      "Episode 1094\tAverage Score: 0.88\n",
      "Environment solved in 1094 episodes!\tAverage Score: 0.88\n",
      "Episode 1095\tAverage Score: 0.91\n",
      "Environment solved in 1095 episodes!\tAverage Score: 0.91\n",
      "Episode 1096\tAverage Score: 0.90\n",
      "Environment solved in 1096 episodes!\tAverage Score: 0.90\n",
      "Episode 1097\tAverage Score: 0.92\n",
      "Environment solved in 1097 episodes!\tAverage Score: 0.92\n",
      "Episode 1098\tAverage Score: 0.94\n",
      "Environment solved in 1098 episodes!\tAverage Score: 0.94\n",
      "Episode 1099\tAverage Score: 0.95\n",
      "Environment solved in 1099 episodes!\tAverage Score: 0.95\n",
      "Episode 1100\tTotal Average Score: 0.94\tDuration: 24.93\n",
      "\n",
      "Environment solved in 1100 episodes!\tAverage Score: 0.94\n",
      "Episode 1101\tAverage Score: 0.96\n",
      "Environment solved in 1101 episodes!\tAverage Score: 0.96\n",
      "Episode 1102\tAverage Score: 0.99\n",
      "Environment solved in 1102 episodes!\tAverage Score: 0.99\n",
      "Episode 1103\tAverage Score: 0.97\n",
      "Environment solved in 1103 episodes!\tAverage Score: 0.97\n",
      "Episode 1104\tAverage Score: 0.97\n",
      "Environment solved in 1104 episodes!\tAverage Score: 0.97\n",
      "Episode 1105\tAverage Score: 0.95\n",
      "Environment solved in 1105 episodes!\tAverage Score: 0.95\n",
      "Episode 1106\tAverage Score: 0.95\n",
      "Environment solved in 1106 episodes!\tAverage Score: 0.95\n",
      "Episode 1107\tAverage Score: 0.98\n",
      "Environment solved in 1107 episodes!\tAverage Score: 0.98\n",
      "Episode 1108\tAverage Score: 0.98\n",
      "Environment solved in 1108 episodes!\tAverage Score: 0.98\n",
      "Episode 1109\tAverage Score: 0.98\n",
      "Environment solved in 1109 episodes!\tAverage Score: 0.98\n",
      "Episode 1110\tAverage Score: 0.97\n",
      "Environment solved in 1110 episodes!\tAverage Score: 0.97\n",
      "Episode 1111\tAverage Score: 1.00\n",
      "Environment solved in 1111 episodes!\tAverage Score: 1.00\n",
      "Episode 1112\tAverage Score: 1.02\n",
      "Environment solved in 1112 episodes!\tAverage Score: 1.02\n",
      "Episode 1113\tAverage Score: 1.03\n",
      "Environment solved in 1113 episodes!\tAverage Score: 1.03\n",
      "Episode 1114\tAverage Score: 1.06\n",
      "Environment solved in 1114 episodes!\tAverage Score: 1.06\n",
      "Episode 1115\tAverage Score: 1.07\n",
      "Environment solved in 1115 episodes!\tAverage Score: 1.07\n",
      "Episode 1116\tAverage Score: 1.10\n",
      "Environment solved in 1116 episodes!\tAverage Score: 1.10\n",
      "Episode 1117\tAverage Score: 1.10\n",
      "Environment solved in 1117 episodes!\tAverage Score: 1.10\n",
      "Episode 1118\tAverage Score: 1.11\n",
      "Environment solved in 1118 episodes!\tAverage Score: 1.11\n",
      "Episode 1119\tAverage Score: 1.11\n",
      "Environment solved in 1119 episodes!\tAverage Score: 1.11\n",
      "Episode 1120\tAverage Score: 1.13\n",
      "Environment solved in 1120 episodes!\tAverage Score: 1.13\n",
      "Episode 1121\tAverage Score: 1.15\n",
      "Environment solved in 1121 episodes!\tAverage Score: 1.15\n",
      "Episode 1122\tAverage Score: 1.18\n",
      "Environment solved in 1122 episodes!\tAverage Score: 1.18\n",
      "Episode 1123\tAverage Score: 1.20\n",
      "Environment solved in 1123 episodes!\tAverage Score: 1.20\n",
      "Episode 1124\tAverage Score: 1.20\n",
      "Environment solved in 1124 episodes!\tAverage Score: 1.20\n",
      "\n",
      "Environment solved in 1124 episodes!\tAverage Score: 1.20\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import time\n",
    "from collections import deque\n",
    "import torch\n",
    "from maddpg_agent import MADDPG\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "agent = MADDPG(action_size=action_size, seed=1)\n",
    "\n",
    "def train(n_episodes=10000, max_t=2000):\n",
    "    scores = []\n",
    "    scores_window = deque(maxlen=100)\n",
    "    scores_avg = []\n",
    "    \n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        Reward = []\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = env_info.vector_observations \n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            actions = agent.act(states, add_noise=True) \n",
    "            env_temp = env.step(actions)[brain_name]\n",
    "            next_states = env_temp.vector_observations\n",
    "            rewards = env_temp.rewards\n",
    "            dones = env_temp.local_done\n",
    "\n",
    "            #for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            agent.step(states, actions, rewards, next_states, dones)\n",
    "            states = next_states\n",
    "            Reward.append(rewards)\n",
    "            if np.any(dones):\n",
    "                break \n",
    "         \n",
    "        # calculate episode reward as maximum of individually collected rewards of agents\n",
    "        episode_reward = np.max(np.sum(np.array(Reward),axis=0))\n",
    "        \n",
    "        scores.append(episode_reward)           \n",
    "        scores_window.append(episode_reward)\n",
    "        average_score = np.mean(scores_window)\n",
    "        scores_avg.append(average_score)\n",
    "        \n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        \n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, average_score), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tTotal Average Score: {:.2f}\\tDuration: {:.2f}'.format(i_episode, average_score, duration))\n",
    "            agent.save_agents()\n",
    "            \n",
    "        if average_score > 0.80:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
    "            agent.save_agents()\n",
    "            #break\n",
    "        if average_score >=1.2:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode, average_score))\n",
    "            agent.save_agents()\n",
    "            break\n",
    "    return scores_avg\n",
    "\n",
    "\n",
    "maddpg = train()\n",
    "\n",
    "# plot the scores\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(maddpg)), maddpg)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
